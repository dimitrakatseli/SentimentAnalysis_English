{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dktlad7QhTA_",
        "outputId": "39127c7d-2731-4d67-8b94-5e5f7b64bcba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/dimitra/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['train.csv', 'test_without_labels.csv', 'sample.csv']\n"
          ]
        }
      ],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"bigdata2020-sentiment/\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kFhpjVVhTBF"
      },
      "source": [
        "# Data Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Spt76xWhTBH",
        "outputId": "a20125bc-7fdb-420f-e46d-e566510b28b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Label</th>\n",
              "      <th>Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Whoever made this movie must have done it as a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love this movie because I grew up around har...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I had some expectation for the movie, since it...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>During the early 1980's, Kurt Thomas was somet...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Poor Will would be rolling over in his grave i...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>After all the crap that Hollywood (and the Ind...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#3 in young John Travolta's trilogy of blockbu...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My first clue about how bad this was going to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I really enjoyed the performances of the main ...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gillian Holroyd (Kim Novak) is a witch. Secret...</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Content  Label  Id\n",
              "0  Whoever made this movie must have done it as a...      0   0\n",
              "1  I love this movie because I grew up around har...      1   1\n",
              "2  I had some expectation for the movie, since it...      0   2\n",
              "3  During the early 1980's, Kurt Thomas was somet...      0   3\n",
              "4  Poor Will would be rolling over in his grave i...      0   4\n",
              "5  After all the crap that Hollywood (and the Ind...      1   5\n",
              "6  #3 in young John Travolta's trilogy of blockbu...      1   6\n",
              "7  My first clue about how bad this was going to ...      0   7\n",
              "8  I really enjoyed the performances of the main ...      1   8\n",
              "9  Gillian Holroyd (Kim Novak) is a witch. Secret...      1   9"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#importing the training and test data\n",
        "imdb_data=pd.read_csv('bigdata2020-sentiment/train.csv/train.csv')\n",
        "test_data=pd.read_csv('bigdata2020-sentiment/test_without_labels.csv/test_without_labels.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvdpP3ihhTBI"
      },
      "outputs": [],
      "source": [
        "#Summary of the dataset\n",
        "#imdb_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NHT1CEphTBJ",
        "outputId": "d132c83f-cf63-496c-adbe-4340ca1ccb7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    12500\n",
              "0    12500\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sentiment count\n",
        "imdb_data['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEvvvbkzhTBJ"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTyZ10-ohTBK"
      },
      "outputs": [],
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_FUsDndhTBL"
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['Content']=imdb_data['Content'].apply(denoise_text)\n",
        "test_data['Content']=test_data['Content'].apply(denoise_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFYjUVgohTBL"
      },
      "outputs": [],
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['Content']=imdb_data['Content'].apply(remove_special_characters)\n",
        "test_data['Content']=test_data['Content'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f08w5mawhTBM"
      },
      "outputs": [],
      "source": [
        "#Lematizing the text\n",
        "def simple_lemma(text):\n",
        "    ps=WordNetLemmatizer()\n",
        "    text= ' '.join([ps.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "##imdb_data['Content']=imdb_data['Content'].apply(simple_lemma)\n",
        "##test_data['Content']=test_data['Content'].apply(simple_lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2oHI2XwhTBN",
        "outputId": "8dace63f-a7f8-415b-984a-9a820249c41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'them', 'by', 'him', 'any', 'this', 'while', 'into', 'mightn', 'other', 'during', \"hasn't\", 'where', \"wasn't\", 'shan', 'we', 'because', 'my', 'in', 'under', 'a', 'does', 'was', 'myself', 'herself', 'more', 'his', 'hadn', 'he', \"she's\", 'theirs', 'how', 'only', 's', 'have', 're', 'having', 'than', 'weren', 'and', 'isn', 'own', 'it', \"you'll\", 'did', 'were', 'here', 'don', 'been', \"it's\", 'yours', 'o', 'has', 'couldn', 'an', \"couldn't\", 'for', 'won', 'as', 'after', 'why', 'yourselves', 'you', 'there', 'their', 'its', 'out', 'few', 'your', 'same', 'hers', 'himself', 'she', 'just', 'between', \"didn't\", 'off', 'further', 'both', 'll', 'mustn', \"needn't\", 'ours', 'wasn', 'over', 'now', 'm', \"won't\", 'with', 'at', 'on', 'will', 'or', \"mightn't\", 'who', 'some', 'each', 'too', 'yourself', 'below', 'do', 'if', 'aren', 'shouldn', 'then', 'whom', \"mustn't\", \"shouldn't\", 'down', 'such', 'ain', 'most', 'all', 'so', 'from', 'hasn', 'haven', 'those', 'should', 'once', 'when', 'needn', \"doesn't\", \"you've\", 'ma', 'until', 'against', \"aren't\", \"you're\", 'me', 'doing', 'had', 'again', 'of', 't', 'up', \"weren't\", 'themselves', 'they', 'd', 'ourselves', 'being', 'our', \"isn't\", 'be', 'above', 've', 'didn', 'very', 'i', \"shan't\", 'is', 'doesn', 'but', 'to', 'can', \"you'd\", 'that', \"that'll\", 'her', 'through', 'itself', 'these', \"hadn't\", \"don't\", \"should've\", 'nor', 'y', \"haven't\", 'the', \"wouldn't\", 'am', 'wouldn', 'which', 'before', 'what', 'about', 'are'}\n"
          ]
        }
      ],
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "stop.remove('no')\n",
        "stop.remove('not')\n",
        "\n",
        "print(stop)\n",
        "#stop.remove('no')\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "imdb_data['Content']=imdb_data['Content'].apply(remove_stopwords)\n",
        "test_data['Content']=test_data['Content'].apply(remove_stopwords)\n",
        "imdb_data['Content']=imdb_data['Content'].apply(simple_lemma)\n",
        "test_data['Content']=test_data['Content'].apply(simple_lemma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9ZkXUNvhTBO",
        "outputId": "4c115af9-21a0-41f8-b709-8ff502455b66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>one best movie seen highly recommend exposure ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>last month seen lot review italian job many ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>movie awful dont even know beginthe positive t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>worst wrestlemania everthis must see bout many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>first saw film theater way back 40 kid always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>another handheld horror mean another divisive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>weird film get impression maker snoozefest spe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>imdb plot summary erroneously make sound like ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>without doubt one neil simon best play turned ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>alice kind movie made 30 40 never attempt even...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                            Content\n",
              "0   0  one best movie seen highly recommend exposure ...\n",
              "1   1  last month seen lot review italian job many ne...\n",
              "2   2  movie awful dont even know beginthe positive t...\n",
              "3   3  worst wrestlemania everthis must see bout many...\n",
              "4   4  first saw film theater way back 40 kid always ...\n",
              "5   5  another handheld horror mean another divisive ...\n",
              "6   6  weird film get impression maker snoozefest spe...\n",
              "7   7  imdb plot summary erroneously make sound like ...\n",
              "8   8  without doubt one neil simon best play turned ...\n",
              "9   9  alice kind movie made 30 40 never attempt even..."
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_data.head(10)\n",
        "test_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrvpsDlghTBO"
      },
      "outputs": [],
      "source": [
        "del imdb_data['Id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpoFN6ZohTBO",
        "outputId": "eb453ed0-82be-4766-9ad4-cb948f126937"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>whoever made movie must done joke mean stupide...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love movie grew around harness racing pat boon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>expectation movie since nice star cast return ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>early 1980s kurt thomas something hero united ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>poor would rolling grave could horiible german...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Content  Label\n",
              "0  whoever made movie must done joke mean stupide...      0\n",
              "1  love movie grew around harness racing pat boon...      1\n",
              "2  expectation movie since nice star cast return ...      0\n",
              "3  early 1980s kurt thomas something hero united ...      0\n",
              "4  poor would rolling grave could horiible german...      0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO0q0PryhTBP",
        "outputId": "3db8e4df-855d-48f9-d469-c2ddd583fdb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOW_cv_train: (23000, 3966181)\n",
            "BOW_cv_test: (2000, 3966181)\n"
          ]
        }
      ],
      "source": [
        "train = imdb_data.iloc[:23000]\n",
        "test = imdb_data.iloc[23000:]\n",
        "real_test = test_data\n",
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "cv_train_reviews=cv.fit_transform(train['Content'])\n",
        "\n",
        "\n",
        "#transformed test reviews\n",
        "cv_test_reviews=cv.transform(test['Content'])\n",
        "real_test_context = cv.transform(real_test['Content'])\n",
        "\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_dh8gbwhTBP"
      },
      "source": [
        "## Multinomial Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoflq0lqhTBQ",
        "outputId": "29c31d79-97f3-4d3c-deb2-34fdb8a09c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
          ]
        }
      ],
      "source": [
        "#training the model\n",
        "mnb=MultinomialNB()\n",
        "#fitting the svm for bag of words\n",
        "mnb_bow=mnb.fit(cv_train_reviews,train['Label'])\n",
        "print(mnb_bow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsoSoqoLhTBR",
        "outputId": "684c544a-70f2-4eb4-be48-74c639f4bb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "#Predicting the model for bag of words\n",
        "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
        "print(mnb_bow_predict)\n",
        "\n",
        "mnb_test_predict = mnb.predict(real_test_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkPFPz_ghTBS",
        "outputId": "4e6a1ce1-4875-4190-e030-b3488875f9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mnb_bow_score : 0.7475\n"
          ]
        }
      ],
      "source": [
        "#Accuracy score for bag of words\n",
        "mnb_bow_score=accuracy_score(test['Label'],mnb_bow_predict)\n",
        "print(\"mnb_bow_score :\",mnb_bow_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNB6SXnhTBS",
        "outputId": "7ad50f41-d55a-4257-9307-eebfa43b702d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.75      0.77      0.76      1030\n",
            "    Negative       0.75      0.72      0.74       970\n",
            "\n",
            "    accuracy                           0.75      2000\n",
            "   macro avg       0.75      0.75      0.75      2000\n",
            "weighted avg       0.75      0.75      0.75      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Classification report for bag of words \n",
        "mnb_bow_report=classification_report(test['Label'],mnb_bow_predict,target_names=['Positive','Negative'])\n",
        "print(mnb_bow_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKftmLBhTBT"
      },
      "source": [
        "## SVM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiSU8WWLhTBT",
        "outputId": "d7da76ae-577f-4eda-b4c1-1895d697384e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
          ]
        }
      ],
      "source": [
        "#training the linear svm\n",
        "svm=SGDClassifier(loss='hinge',random_state=42)\n",
        "#fitting the svm for bag of words\n",
        "svm_bow=svm.fit(cv_train_reviews,train['Label'])\n",
        "print(svm_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVBS-HtdhTBU",
        "outputId": "58c74e88-f221-427b-c8d2-6ca209c939f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 ... 1 1 1]\n",
            "[0 0 0 ... 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "svm_bow_predict=svm.predict(cv_test_reviews)\n",
        "print(svm_bow_predict)\n",
        "\n",
        "svm_test_final = svm.predict(real_test_context)\n",
        "print(svm_test_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENEVzHF1hTBU",
        "outputId": "3a271dcc-c728-480e-9f80-c5d018d1e625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm_bow_score : 0.749\n"
          ]
        }
      ],
      "source": [
        "svm_bow_score=accuracy_score(test['Label'],svm_bow_predict)\n",
        "print(\"svm_bow_score :\",svm_bow_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0-oRPv9hTBW",
        "outputId": "411ca50f-257d-49c7-e694-27b8eba262f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Positive       0.79      0.69      0.74      1030\n",
            "    Negative       0.71      0.81      0.76       970\n",
            "\n",
            "    accuracy                           0.75      2000\n",
            "   macro avg       0.75      0.75      0.75      2000\n",
            "weighted avg       0.75      0.75      0.75      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Classification report for bag of words \n",
        "svm_bow_report=classification_report(test['Label'],svm_bow_predict,target_names=['Positive','Negative'])\n",
        "print(svm_bow_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKdZMtK4hTBW",
        "outputId": "8c349deb-5cf4-4c70-969a-b3c79c444132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[783 187]\n",
            " [315 715]]\n"
          ]
        }
      ],
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test['Label'],svm_bow_predict,labels=[1,0])\n",
        "print(cm_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujQIsU6UhTBX"
      },
      "outputs": [],
      "source": [
        "prediction = pd.DataFrame(svm_test_final, columns=['Predicted']).to_csv('svm_prediction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtzk-G_mhTBX"
      },
      "source": [
        "# Keras LSTM model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuVByE_dhTBX"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Convolution1D\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28UsZ3PUhTBY"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(imdb_data['Content'])\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(imdb_data['Content'])\n",
        "maxlen = 500\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "y = imdb_data['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM_iASyJhTBY"
      },
      "outputs": [],
      "source": [
        "embed_size = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_size))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6OHG9K4hTBZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTW6vVQAhTBZ",
        "outputId": "a4b50f54-4631-4a4f-c53b-26ded9c4d55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, None, 300)         3000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_15 (Bidirectio (None, None, 64)          85248     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_15 (Glo (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 20)                1300      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 3,086,989\n",
            "Trainable params: 3,086,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsKc8twShTBZ",
        "outputId": "f1ad095c-6a4d-4e54-b7f4-d8c91ee06f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 23750 samples, validate on 1250 samples\n",
            "Epoch 1/2\n",
            "23750/23750 [==============================] - 201s 8ms/step - loss: 0.4130 - acc: 0.8206 - val_loss: 0.2848 - val_acc: 0.8784\n",
            "Epoch 2/2\n",
            "23750/23750 [==============================] - 195s 8ms/step - loss: 0.2016 - acc: 0.9276 - val_loss: 0.3123 - val_acc: 0.8856\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f26ceda8320>"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF4vW4sehTBa"
      },
      "outputs": [],
      "source": [
        "list_sentences_test = test_data['Content']\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "predictions = model.predict(X_te)\n",
        "y_pred = (predictions > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44n8V9HKhTBa"
      },
      "outputs": [],
      "source": [
        "y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8LWwBsRhTBa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTKgtg3KhTBb"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XipdKAahTBb"
      },
      "outputs": [],
      "source": [
        "prediction = pd.DataFrame(y_pred, columns=['Predicted']).to_csv('prediction28.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A11iJfU2hTBb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Sentiment Analysis .ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}